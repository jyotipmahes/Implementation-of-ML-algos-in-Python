{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree is a type of **supervised learning** algorithm that works both for **regression** and **classification** problems. It works for both **categorical and continuous input and output variables**. In this technique, we **split** the population or sample **into two or more homogeneous sets** (or sub-populations) based on most significant splitter / differentiator in input variables. Decision Tree's are really popular because they are very easy to **interpret** and forms the basis for more complex bagging techniques such as **Random Forest**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why binary split are favoured?** <br>\n",
    "Generally, we use binary tree to split the tree instead of n-split tree as binary tree's much more easy to implement and are very efficient. Also, any n-split can be easily achieved even with multiple binary splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Terminologies\n",
    "Let’s look at the basic terminology used with Decision trees:\n",
    "\n",
    "**Root Node:** It represents entire population or sample and this further gets divided into two or more homogeneous sets.<br>\n",
    "**Splitting:** It is a process of dividing a node into two or more sub-nodes. <br>\n",
    "**Decision Node:** When a sub-node splits into further sub-nodes, then it is called decision node.<br>\n",
    "**Leaf/ Terminal Node:** Nodes do not split is called Leaf or Terminal node.<br>\n",
    "**Pruning:** When we remove sub-nodes of a decision node, this process is called pruning. You can say opposite process of splitting.<br>\n",
    "**Branch / Sub-Tree:** A sub section of entire tree is called branch or sub-tree.<br>\n",
    "**Parent and Child Node:** A node, which is divided into sub-nodes is called parent node of sub-nodes where as sub-nodes are the child of parent node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://www.analyticsvidhya.com/wp-content/uploads/2015/01/Decision_Tree_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Criterion\n",
    "The decision of making strategic splits **heavily affects a tree’s accuracy**. The decision **criteria is different for classification and regression trees**.The creation of sub-nodes increases the **homogeneity** of resultant sub-nodes. In other words, we can say that **purity of the node increases** with respect to the target variable. Decision tree splits the nodes on **all available variables** and then selects the split which results in **most homogeneous sub-nodes**. Refer to following [blog](https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/) for detailed explanation.\n",
    "\n",
    "The algorithm selection is also based on **type of target variables**. Let’s look at the four most commonly used algorithms in decision tree:\n",
    "\n",
    "**Gini Index**: Gini index says, if we select two items from a population at random then they must be of same class and probability for this is 1 if population is pure.\n",
    "1. It works with categorical target variable “Success” or “Failure” and  performs only Binary splits\n",
    "\n",
    "Steps to Calculate Gini for a split\n",
    "1. Calculate Gini for sub-nodes, using formula sum of square of probability for success and failure (p^2+q^2).\n",
    "2. Calculate Gini for split using weighted Gini score of each node of that split.<br>\n",
    "\n",
    "![title](https://cdn-images-1.medium.com/max/800/1*2bI1Uxv3bBzR9Nkg_PLiaw.png)\n",
    "![title](https://cdn-images-1.medium.com/max/800/1*kvIc3ZwDyYab3GOOKY5lHw.png)\n",
    "\n",
    "**Node split happens for feature with higher Gini Score**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chi-Square**:It is an algorithm to find out the statistical significance between the differences between sub-nodes and parent node. We measure it by sum of squares of standardized differences between observed and expected frequencies of target variable.\n",
    "1. It works with categorical target variable “Success” or “Failure” and can perform two or more splits.\n",
    "2. Higher the value of Chi-Square higher the statistical significance of differences between sub-node and Parent node.\n",
    "3. Chi-square = ((Actual – Expected)^2 / Expected)^1/2\n",
    "\n",
    "Steps to Calculate Chi-square for a split:\n",
    "\n",
    "1. Calculate Chi-square for individual node by calculating the deviation for Success and Failure both\n",
    "2. Calculated Chi-square of Split using Sum of all Chi-square of success and Failure of each node of the split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Information Gain:** Information gain is derived in terms of entropy. Entropy is the degree of disorganization in a system. If the sample is completely homogeneous, then the entropy is zero and if the sample is an equally divided (50% – 50%), it has entropy of one. Entropy is given by the formula:\n",
    "$$ E = -p\\log_2(p) - q\\log_2(q)$$\n",
    "\n",
    "Here p and q is probability of success and failure respectively in that node. Entropy is also used with categorical target variable. It chooses the split which has lowest entropy compared to parent node and other splits. The lesser the entropy, the better it is.\n",
    "\n",
    "Steps to calculate entropy for a split:\n",
    "\n",
    "1. Calculate entropy of parent node\n",
    "2. Calculate entropy of each individual node of split and calculate weighted average of all sub-nodes available in split.\n",
    "3. The difference between parent and sub-node is called Information gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reduction in Variance** Reduction in variance is an algorithm used for continuous target variables (regression problems). This algorithm uses the standard formula of variance to choose the best split. The split with lower variance is selected as the criteria to split the population:\n",
    "\n",
    "$$ var = \\sum\\frac{(X-\\bar(X))^2}{n-1}$$\n",
    "\n",
    "Steps to calculate Variance:\n",
    "\n",
    "1. Calculate variance for each node.\n",
    "2. Calculate variance for each split as weighted average of each node variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopping Criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting is one of the key challenges faced while modeling decision trees. Preventing overfitting is pivotal while modeling a decision tree and it can be done in 2 ways:\n",
    "\n",
    "### 1. Setting constraints on tree size: \n",
    "This can be done by using various parameters which are used to define a tree. \n",
    "\n",
    "**Minimum samples for a node split**: Defines the minimum number of samples (or observations) which are required in a node to be considered for splitting.<br>\n",
    "**Minimum samples for a terminal node (leaf)**:Defines the minimum samples (or observations) required in a terminal node or leaf.Generally lower values should be chosen for imbalanced class problems because the regions in which the minority class will be in majority will be very small. <br>\n",
    "**Maximum depth of a tree**: Used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample.<br>\n",
    "**Maximum number of terminal nodes**:The maximum number of terminal nodes or leaves in a tree. Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves.<br>\n",
    "**Maximum features to consider for split:** The number of features to consider while searching for a best split. These will be randomly selected. As a thumb-rule, square root of the total number of features works great but we should check upto 30-40% of the total number of features.\n",
    "\n",
    "### 2. Tree pruning:\n",
    "We grow the tree and then remove splits which does not given good gains. Currently not present in sklearn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of decision Tree\n",
    "Key Components: \n",
    "1. Initialize a tree\n",
    "2. Implement splitting criterion\n",
    "3. Find best split for a feature\n",
    "4. Find best split for across multiple features\n",
    "4. Create branches/lower nodes\n",
    "5. Prediction function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Decision tree classifier with Gini Index\n",
    "We will start by implementing a decision tree classifier with Gini Index. Splitting criterion can be changed easily by just changing splitting criterion function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Initialize a tree\n",
    "We **initialize** a tree with **max depth**, **current depth**, **left side node and right side node**. We **can add more** attributes if we want. For given implementation, we will go ahead with this. We **will slightly change it during Regression model** to show some variations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import mode\n",
    "\n",
    "class DecisionTree:\n",
    "\n",
    "    def __init__(self, max_depth = 5, depth = 1):\n",
    "        self.max_depth = max_depth\n",
    "        self.depth = depth\n",
    "        self.left = None\n",
    "        self.right = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implement splitting criterion(Gini Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __calculate_impurity_score(self, data):\n",
    "    if data is None or data.empty: return 0\n",
    "    gini_impurity = 1.\n",
    "    classes = np.unique(data)\n",
    "    n_vals = len(data)\n",
    "    for cl in classes:\n",
    "        gini_impurity -= np.square(np.sum(data==cl)/n_vals)  # subtracting sum of square probability from 1\n",
    "    return gini_impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is the impurity calculation. To complete the splitting, we will also need the information gain information, i.e weighted difference in Gini score between parent and child nodes. Note **__** is used before function name as they are meant to be **internal functions(in a sense private functions)** which we do not want to call from outside. Since, **python does not have** a concept of **private vs public functions**, we use following convention for ease of understanding. <br>\n",
    "\n",
    "Here is the implementation for **Information Gain**. We **subtract** the **weighted child impurity score from Parent node**. `self.impurity_score` already has parent gini score stored(see in full code) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __calculate_information_gain(self, left_count, left_impurity, right_count, right_impurity):\n",
    "    return self.impurity_score - ((left_count/len(self.data)) * left_impurity + \\\n",
    "                                  (right_count/len(self.data)) * right_impurity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Find best split for a feature column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For tree splitting on a given column, we do following things:\n",
    "1. Find the unique values in the column as we split on the column values.\n",
    "2. If there is only 1 unique value, no need to split as it is already pure node.\n",
    "2. Run a loop on all unique values as split point\n",
    "3. Values less than equal to split goes to left and larger values goes to right. This way we create a binary split.\n",
    "4. Calculate the impurity score and information gain based on above split.\n",
    "5. Store the split which has maximum information gain.\n",
    "\n",
    "**Note that information gain is always >= 0 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __find_best_split_for_column(self, col):\n",
    "    x = self.data[col]\n",
    "    unique_values = x.unique()\n",
    "    if len(unique_values) == 1: return None, None\n",
    "    information_gain = None\n",
    "    split = None\n",
    "    for val in unique_values:\n",
    "        left = x <= val\n",
    "        right = x > val\n",
    "        left_data = self.data[left]\n",
    "        right_data = self.data[right]\n",
    "        left_impurity = self.__calculate_impurity_score(left_data[self.target])\n",
    "        right_impurity = self.__calculate_impurity_score(right_data[self.target])\n",
    "        score = self.__calculate_information_gain(left_count = len(left_data),\n",
    "                                                  left_impurity = left_impurity,\n",
    "                                                  right_count = len(right_data),\n",
    "                                                  right_impurity = right_impurity)\n",
    "        if information_gain is None or score > information_gain: \n",
    "            information_gain = score \n",
    "            split = val\n",
    "    return information_gain, split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Find best split for across multiple features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a wrapper function on top of `__find_best_split_for_column` to find the best column and split value across different columns/features. Note that all features are available at every node although we split on specific column/feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __find_best_split(self):\n",
    "    best_split = {}\n",
    "    for col in self.columns:\n",
    "        information_gain, split = self.__find_best_split_for_column(col)\n",
    "        if split is None: continue\n",
    "        if not best_split or best_split[\"information_gain\"] < information_gain:\n",
    "            best_split = {\"split\": split, \"col\": col, \"information_gain\": information_gain}\n",
    "\n",
    "    return best_split.get(\"split\"), best_split.get(\"col\"), best_split.get(\"information_gain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create branches/lower nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have found the best column/feature and split value to split on, we split the node into two child nodes or we grow the tree downwards. Here are the steps:\n",
    "1. Initialize two new nodes of type `DecisionTree` called as `left` and `right`.\n",
    "2. Using split column/feature and split value, divide the data as `left_rows` and `right_rows`.\n",
    "3. Call fit function to create and initialize tree branches.\n",
    "\n",
    "We will discuss `self.fit` next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __create_branches(self):\n",
    "    self.left = DecisionTree(max_depth = self.max_depth, \n",
    "                             depth = self.depth + 1)\n",
    "    self.right = DecisionTree(max_depth = self.max_depth, \n",
    "                             depth = self.depth + 1)\n",
    "    left_rows = self.data[self.data[self.split_feature] <= self.criteria] \n",
    "    right_rows = self.data[self.data[self.split_feature] > self.criteria] \n",
    "    self.left.fit(data = left_rows, target = self.target)\n",
    "    self.right.fit(data = right_rows, target = self.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit function\n",
    "Once we have created the `DecisionTree` **constructor**, we need to call the **fit** function with the **data and target column** to start fitting a `DecisionTree` on the **training data**. Below is the definition of the fit function. It does following things:\n",
    "1. It takes the `data` and `name of the target column(str)` as input.\n",
    "2. It initializes `columns` with the name of the feature columns.\n",
    "3. If `max depth` of the tree is not reached, it validates the data to check it is in correct format.\n",
    "4. It calculates impurity score for the parent node and finds best split and performs splitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, data, target):\n",
    "    self.data = data\n",
    "    self.target = target\n",
    "    self.columns = self.data.columns.tolist()\n",
    "    self.columns.remove(target)\n",
    "    if self.depth <= self.max_depth:\n",
    "        self.__validate_data()\n",
    "        self.impurity_score = self.__calculate_impurity_score(self.data[self.target])\n",
    "        self.criteria, self.split_feature, self.information_gain = self.__find_best_split()\n",
    "        if self.criteria is not None and self.information_gain > 0: self.__create_branches()\n",
    "    else: \n",
    "        print(\"Stopping splitting as Max depth reached\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data validation function\n",
    "**DecisionTree** needs all **columns values to numeric**. It check this condition and raises error if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __validate_data(self):\n",
    "    non_numeric_columns = self.data[self.columns].select_dtypes(include=['category', 'object', 'bool']).columns.tolist()\n",
    "    if(len(set(self.columns).intersection(set(non_numeric_columns))) != 0):\n",
    "        raise RuntimeError(\"Not all columns are numeric\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Prediction function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a decision tree, the **predictions are made at the leaves**, i.e nodes which has no childs. So, in order to make prediction for a test point, we need to traverse the tree based on split logics and reach the corresponding leaf. Once we reach leaf, we can make prediction in a variety of ways:\n",
    "1. Predict based on majority voting of target observed during training\n",
    "2. Take weighted vote\n",
    "3. So other custom function to find the prediction. \n",
    "\n",
    "For now, we will go ahead with majority voting. In order to implement predict function, we first need following things:\n",
    "\n",
    "**a) Check if a node is leaf or not**:<br>\n",
    "\n",
    "We do this by defining a **class property** as shown below. It **checks** if the node has any **left branch** or not. It is also **equivalent** to **checking right or both** as we always do **binary splits.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@property\n",
    "def is_leaf_node(self): return self.left is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note `property` can be thought as a way of using a function as an attribute which can help in getting(getter) or setting(setter) and value stored in the class object** Follow this [link](https://www.programiz.com/python-programming/property) to better understanding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) Majority vote as prediction**:<br?\n",
    "We implement this as well using class property as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@property\n",
    "def prediction(self): \n",
    "    return mode(self.data[self.target])[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c) Walk through the tree till reach node and make prediction**\n",
    "We define a function, which **recursively transverse** the tree and make prediction once leaf is reached as shown below. At every point we **check the splitting criterion and go to left or right** based on where our test point lies in feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __flow_data_thru_tree(self, row):\n",
    "    if self.is_leaf_node: return self.prediction\n",
    "    tree = self.left if row[self.split_feature] <= self.criteria else self.right\n",
    "    return tree.__flow_data_thru_tree(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d) Wrapper Function to predict for all test points**\n",
    "Following function, makes prediction for all test points given for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, data):\n",
    "    return np.array([self.__flow_data_thru_tree(row) for _, row in data.iterrows()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Implementation for DecisonTree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class DecisionTree:\n",
    "\n",
    "    def __init__(self, max_depth = 5, depth = 1):\n",
    "        self.max_depth = max_depth\n",
    "        self.depth = depth\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "    \n",
    "    def fit(self, data, target):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.columns = self.data.columns.tolist()\n",
    "        self.columns.remove(target)\n",
    "        if self.depth <= self.max_depth:\n",
    "            self.__validate_data()\n",
    "            self.impurity_score = self.__calculate_impurity_score(self.data[self.target])\n",
    "            self.criteria, self.split_feature, self.information_gain = self.__find_best_split()\n",
    "            if self.criteria is not None and self.information_gain > 0: self.__create_branches()\n",
    "        else: \n",
    "            print(\"Stopping splitting as Max depth reached\")\n",
    "    \n",
    "    def __create_branches(self):\n",
    "        self.left = DecisionTree(max_depth = self.max_depth, \n",
    "                                 depth = self.depth + 1)\n",
    "        self.right = DecisionTree(max_depth = self.max_depth, \n",
    "                                 depth = self.depth + 1)\n",
    "        left_rows = self.data[self.data[self.split_feature] <= self.criteria] \n",
    "        right_rows = self.data[self.data[self.split_feature] > self.criteria] \n",
    "        self.left.fit(data = left_rows, target = self.target)\n",
    "        self.right.fit(data = right_rows, target = self.target)\n",
    "    \n",
    "    def __calculate_impurity_score(self, data):\n",
    "        if data is None or data.empty: return 0\n",
    "        gini_impurity = 1.\n",
    "        classes = np.unique(data)\n",
    "        n_vals = len(data)\n",
    "        for cl in classes:\n",
    "            gini_impurity -= np.square(np.sum(data==cl)/n_vals)  # substarcting sum of square probability from 1\n",
    "        return gini_impurity\n",
    "    \n",
    "    def __find_best_split(self):\n",
    "        best_split = {}\n",
    "        for col in self.columns:\n",
    "            information_gain, split = self.__find_best_split_for_column(col)\n",
    "            if split is None: continue\n",
    "            if not best_split or best_split[\"information_gain\"] < information_gain:\n",
    "                best_split = {\"split\": split, \"col\": col, \"information_gain\": information_gain}\n",
    "\n",
    "        return best_split.get(\"split\"), best_split.get(\"col\"), best_split.get(\"information_gain\")\n",
    "\n",
    "    def __find_best_split_for_column(self, col):\n",
    "        x = self.data[col]\n",
    "        unique_values = x.unique()\n",
    "        if len(unique_values) == 1: return None, None\n",
    "        information_gain = None\n",
    "        split = None\n",
    "        for val in unique_values:\n",
    "            left = x <= val\n",
    "            right = x > val\n",
    "            left_data = self.data[left]\n",
    "            right_data = self.data[right]\n",
    "            left_impurity = self.__calculate_impurity_score(left_data[self.target])\n",
    "            right_impurity = self.__calculate_impurity_score(right_data[self.target])\n",
    "            score = self.__calculate_information_gain(left_count = len(left_data),\n",
    "                                                      left_impurity = left_impurity,\n",
    "                                                      right_count = len(right_data),\n",
    "                                                      right_impurity = right_impurity)\n",
    "            if information_gain is None or score > information_gain: \n",
    "                information_gain = score \n",
    "                split = val\n",
    "        return information_gain, split\n",
    "    \n",
    "    def __calculate_information_gain(self, left_count, left_impurity, right_count, right_impurity):\n",
    "        return self.impurity_score - ((left_count/len(self.data)) * left_impurity + \\\n",
    "                                      (right_count/len(self.data)) * right_impurity)\n",
    "\n",
    "    def predict(self, data):\n",
    "        return np.array([self.__flow_data_thru_tree(row) for _, row in data.iterrows()])\n",
    "\n",
    "    def __validate_data(self):\n",
    "        non_numeric_columns = self.data[self.columns].select_dtypes(include=['category', 'object', 'bool']).columns.tolist()\n",
    "        if(len(set(self.columns).intersection(set(non_numeric_columns))) != 0):\n",
    "            raise RuntimeError(\"Not all columns are numeric\")\n",
    "\n",
    "    def __flow_data_thru_tree(self, row):\n",
    "        if self.is_leaf_node: return self.probability\n",
    "        tree = self.left if row[self.split_feature] <= self.criteria else self.right\n",
    "        return tree.__flow_data_thru_tree(row)\n",
    "        \n",
    "    @property\n",
    "    def is_leaf_node(self): return self.left is None\n",
    "\n",
    "    @property\n",
    "    def probability(self): \n",
    "        return mode(self.data[self.target])[0][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check implementation on iris data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "iris_df['target'] = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping splitting as Max depth reached\n",
      "Stopping splitting as Max depth reached\n",
      "Stopping splitting as Max depth reached\n",
      "Stopping splitting as Max depth reached\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTree(max_depth= 3)\n",
    "clf.fit(iris_df, 'target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.973\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy %.3f\" %(np.sum(clf.predict(iris_df)==iris.target)*1.0/len(iris.target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing to Sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "skclf = DecisionTreeClassifier(max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skclf.fit(X=iris.data, y=iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.973\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy %.3f\" %(np.sum(skclf.predict(iris.data)==iris.target)*1.0/len(iris.target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can see that we get the same accuracy showing our implementation is correct.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros and Cons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pros**\n",
    "1. Easy to Understand\n",
    "2. Useful in Data exploration\n",
    "3. Less data cleaning required\n",
    "4. Data type is not a constraint\n",
    "5. Non Parametric Method\n",
    "\n",
    "**Cons**\n",
    "1. Over fitting\n",
    "2. Not great for continuous variables\n",
    "\n",
    "Check this [blog](https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/) for detailed explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variations to try\n",
    "1. **Regression:** We will implement in a different notebook\n",
    "2. Try **different impurity** calculation like entropy\n",
    "3. **Do beam search:** We have implemented a greedy algo which looks only at immediate split to find best split. We will implement beam search which looks at next `n` splits to find best split in a different notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
